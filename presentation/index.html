<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Don't trust your compiler to optimize</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<!-- Append e.g. ?print-pdf&showNotes=separate-page to URL -->
		<script>
			// Switch to light theme for PDF/printing
			if (window.location.search.match( /print-pdf/gi ))
			{
				var link_theme = document.createElement( 'link' );
				link_theme.rel = 'stylesheet';
				link_theme.type = 'text/css';
				link_theme.href = 'css/theme/white.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link_theme );

				var link_highlight_theme = document.createElement( 'link' );
				link_highlight_theme.rel = 'stylesheet';
				link_highlight_theme.type = 'text/css';
				link_highlight_theme.href = 'highlight-vs.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link_highlight_theme );
			}

			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section><h2>Don't trust your compiler to optimize</h2>
				<br />
				<p>Malmö C++ User Group meeting
				<p>January 2019</p>
				<br />
				<p>Reimar Döffinger</p>
				<p><small><small><a href="?print-pdf">Print view</a> or <a href="?print-pdf&amp;showNotes=separate-page">print view with notes</a> (may need Chrome/Chromium)</small></small></p>
				</section>

				<section><h2>Introduction</h2>
					<ul>
					<li>Examples of badly performing generated code</li>
					<li>Based on gcc - only because of familiarity</li>
					<li>Compilers are great - but know their flaws</li>
					</ul>
				<aside class="notes">
					<p>Not meant to blame compiler developers!</p>
					<p>gcc: really good quality, not encountered a compiler bug in years!</p>
					<p>Should be thankful for that</p>
					<br />
					<p>Compilers have improved a lot...</p>
					<p>...yet nothing fundamentally changed</p>
					<p>"Obviously stupid" code still common with any compiler</p>
					<p>Because compilers are not human and do not know "obviously stupid"</p>
					<p>Great at compiling huge projects without error - failing at not doing trivial things in horribly complex ways</p>
				</aside>
				</section>

				<section>
				<section>
				<h2>Initialization</h2>
				<pre><code data-trim class="cpp">
struct cleartest
{
  cleartest() : a(0), b(0) {}
  long long a;
  int b;
};
cleartest tmp[128];
				</code></pre>
				<p>What do you expect compiler to produce?</p>
				<aside class="notes">
					This is a simplified case. new[] has similar issues and affects more than just startup.
				</aside>
				</section>
				<section>
				<h2>Initialization</h2>
				<p>gcc 8</p>
				<pre><code data-trim class="x86asm">
        .text
        .section        .text.startup,"ax",@progbits
        leaq    tmp(%rip), %rax
        leaq    2048(%rax), %rdx
.L2:    movq    $0, (%rax)   # long long a
        addq    $16, %rax
        movl    $0, -8(%rax) # int b
        cmpq    %rdx, %rax
        jne     .L2
        ret
	.align 32
        .bss
tmp:
        .zero   2048

				</code></pre>
				</section>
				<section>
				<h2>Initialization</h2>
				<p>clang 7</p>
				<pre><code data-trim class="x86asm">
        .bss
        .p2align        4
tmp:
        .zero   2048
				</code></pre>
				</section>
				</section>

				<section>
				<section>
				<h2>Initialization (2)</h2>
				<pre><code data-trim class="cpp">
struct cleartest
{
  cleartest() : a(0), b(0) {}
  long long a;
  long long b;
};
cleartest tmp[128];
				</code></pre>
				<p>And now?</p>
				<aside class="notes">
					This example avoids the padding.
				</aside>
				</section>
				<section>
				<h2>Initialization (2)</h2>
				<p>gcc 8</p>
				<pre><code data-trim class="x86asm">
        leaq    tmp(%rip), %rax
        pxor    %xmm0, %xmm0
        leaq    2048(%rax), %rdx
.L2:
        movaps  %xmm0, (%rax)  # long long a, b
        addq    $16, %rax
        cmpq    %rax, %rdx
        jne     .L2
        ret
				</code></pre>
				<p>Well, at least some SSE is now used...</p>
				</section>
				</section>

				<section>
				<section>
				<h2>Initialization (3)</h2>
				<pre><code data-trim class="cpp">
struct cleartest
{
  cleartest() : a(0), b(0), c(1), d(0) {}
  int a;
  int b;
  short c;
  short d;
};

cleartest tmp[128];
				</code></pre>
				<p>How will this one differ?</p>
				<aside class="notes">
					Not all 0s, so just dumping it in .bss without initialization code as clang did before cannot work.
				</aside>
				</section>
				<section>
				<h2>Initialization (3)</h2>
				<p>gcc 8</p>
				<pre><code data-trim class="x86asm">
        leaq    tmp(%rip), %rax
        leaq    1536(%rax), %rdx
.L2:    movl    $1, %ecx
        xorl    %esi, %esi
        addq    $12, %rax
        movl    $0, -12(%rax)  # int a
        movl    $0, -8(%rax)   # int b
        movw    %cx, -4(%rax)  # short c = 1
        movw    %si, -2(%rax)  # short d
        cmpq    %rdx, %rax
        jne     .L2
        ret
				</code></pre>
				<p>See the xorl in the loop and weep...</p>
				<aside class="notes">
					Also none of the writes merged, no loop unrolling etc.
				</aside>
				</section>
				<section>
				<h2>Initialization (3)</h2>
				<p>clang 7</p>
				<pre><code data-trim class="x86asm">
        movl    $tmp, %eax
        movl    $tmp+1536, %ecx
.LBB0_1: movq    $0, (%rax)  # int a, b
        movl    $1, 8(%rax)  # short c, d
        movq    $0, 12(%rax) # int a, b
        movl    $1, 20(%rax) # short c, d
        movq    $0, 24(%rax)
        movl    $1, 32(%rax)
        movq    $0, 36(%rax)
        movl    $1, 44(%rax)
        addq    $48, %rax
        cmpq    %rcx, %rax
        jne     .LBB0_1
        retq
				</code></pre>
				<p>Better, but still no SSE, not in .data</p>
				</section>
				</section>

				<section>
				<section>
				<h2>Assignment</h2>
				<pre><code data-trim class="cpp">
struct test { int a, b, c, d; ... };
std::array&lt;test, 500&gt; alarge, blarge;
std::array&lt;test, 100&gt; amed, bmed;
std::array&lt;test, 1&gt; atiny, btiny;
void assignlarge() { alarge = blarge; }
void assignmed() { amed = bmed; }
void assigntiny() { atiny = btiny; }
				</code></pre>
				<p>Will array size affect the generated code? struct is 56 bytes large.</p>
				</section>
				<section>
				<h2>Assignment</h2>
				<p>clang 7, large size</p>
				<pre><code data-trim class="x86asm">
_Z11assignlargev:
        movl    $alarge, %edi
        movl    $blarge, %esi
        movl    $28000, %edx            # imm = 0x6D60
        jmp     memcpy                  # TAILCALL
_Z9assignmedv:
        movl    $amed, %edi
        movl    $bmed, %esi
        movl    $5600, %edx             # imm = 0x15E0
        jmp     memcpy                  # TAILCALL
				</code></pre>
				<p>Simply uses memcpy, benefits from libc optimizations</p>
				<aside class="notes">
					Allows to benefit from memcpy being resolved to the optimal function for the current CPU, and future libc updates might even use newer instruction that do not yet exist!
				</aside>
				</section>
				<section>
				<h2>Assignment</h2>
				<p>clang 7, small size</p>
				<pre><code data-trim class="x86asm">
_Z10assigntinyv:
        movq    btiny+48(%rip), %rax
        movq    %rax, atiny+48(%rip)
        movups  btiny+32(%rip), %xmm0
        movups  %xmm0, atiny+32(%rip)
        movups  btiny+16(%rip), %xmm0
        movups  %xmm0, atiny+16(%rip)
        movups  btiny(%rip), %xmm0
        movups  %xmm0, atiny(%rip)
        retq
				</code></pre>
				<p>Inlined memcpy, but is it optimal?</p>
				</section>
				<section>
				<h2>Assignment</h2>
				<p>gcc 8, small size</p>
				<pre><code data-trim class="x86asm">
_Z10assigntinyv:
        movdqa  btiny(%rip), %xmm0
        movdqa  16+btiny(%rip), %xmm1
        movdqa  32+btiny(%rip), %xmm2
        movq    48+btiny(%rip), %rax
        movaps  %xmm0, atiny(%rip)
        movaps  %xmm1, 16+atiny(%rip)
        movaps  %xmm2, 32+atiny(%rip)
        movq    %rax, 48+atiny(%rip)
        ret
				</code></pre>
				<p>Also nicely aligned! Different (better?) scheduling!</p>
				<aside class="notes">
				Clang for some reason does not align atiny/btiny sufficiently to be able to use aligned movs.
				</aside>
				</section>
				<section>
				<h2>Assignment</h2>
				<p>gcc 8, large size</p>
				<pre><code data-trim class="x86asm">
_Z11assignlargev:
        subq    $8, %rsp
        movl    $28000, %edx
        leaq    blarge(%rip), %rsi
        leaq    alarge(%rip), %rdi
        call    memcpy@PLT
        addq    $8, %rsp
        ret
_Z9assignmedv:
        leaq    amed(%rip), %rdi
        leaq    bmed(%rip), %rsi
        movl    $700, %ecx
        rep movsq
        ret
				</code></pre>
				<p>Looks a lot less nice...</p>
				<aside class="notes">
				No tailcall optimization for memcpy, and 700 iterations of rep movsq depends on architecture if it works well! Well optimized on some architectures (but with large setup overhead).
				</aside>
				</section>
				</section>


				<section>
				<section>
				<h2>Signed Arithmetic</h2>
				<pre><code data-trim class="cpp">
unsigned long long t(int i) {
  return 2*(long long)(2*i);
}

unsigned long long t2(int i) {
  return 2*(unsigned long long)(2*i);
}

short t3(short *a, int i) {
  return a[2*i];
}
				</code></pre>
				<p>What is the difference between these?</p>
				<p>How many instructions are needed?</p>
				<aside class="notes">
				These do almost the same thing. Note signed overflow is undefined, unsigned is not. In all cases 2*i is signed though, but the cast seems to confuse compiler.
				</aside>
				</section>
				<section>
				<h2>Signed Arithmetic</h2>
				<p>gcc 8</p>
				<pre><code data-trim class="x86asm">
_Z1ti:  movslq  %edi, %rax          # 2*(s64)(2*i)
        salq    $2, %rax
        ret
_Z2t2i: leal    (%rdi,%rdi), %eax   # 2*(u64)(2*i)
        cltq
        addq    %rax, %rax
        ret
_Z2t3Psi:                           # a[2*i]
        addl    %esi, %esi
        movslq  %esi, %rsi
        movzwl  (%rdi,%rsi,2), %eax
        ret
				</code></pre>
				<aside class="notes">
				Since signed overflow is undefined, both multiplications can be merged in the first case.
				In the second case they are done separately with extension to 64-bit in-between as if it was a unsigned multiply...
				However for array indexing gcc suddenly forgets it can merge the multiplications.
				</aside>
				</section>
				<section>
				<h2>Signed Arithmetic</h2>
				<p>clang 7</p>
				<pre><code data-trim class="x86asm">
_Z1ti:  addl    %edi, %edi
        movslq  %edi, %rax
        addq    %rax, %rax
        retq
_Z2t2i: addl    %edi, %edi
        movslq  %edi, %rax
        addq    %rax, %rax
        retq
_Z2t3Psi:
        addl    %esi, %esi
        movslq  %esi, %rax
        movzwl  (%rdi,%rax,2), %eax
        retq
				</code></pre>
				<aside class="notes">
				clang doesn't know about any such optimizations. Also a bit different instruction selection.
				</aside>
				</section>
				</section>

				<section>
				<section>
				<h2>Multiplication</h2>
				<pre><code data-trim class="cpp">
unsigned short mul(unsigned short x, unsigned char t)
{
    return x * t;
}

unsigned long mullarge(unsigned char x, unsigned char t)
{
    return (unsigned long)x * t;
}
				</code></pre>
				<p>Not much could go wrong here, right?</p>
				<p>So let's also look at other architectures</p>
				</section>
				<section>
				<h2>Multiplication</h2>
				<p>avr-gcc 5.4.0</p>
				<pre><code data-trim class="x86asm">
_Z3multh: movw r18,r24    # u16 = u16 * u8
        mul r22,r18
        movw r24,r0
        mul r22,r19
        add r25,r0
        clr __zero_reg__
        ret
_Z8mullargehh:            # u32 = (u8 cast to u32) * u8
        mov r18,r24
        ldi r19,0
        mov r26,r22
        ldi r27,0
	call __umulhisi3 # 16x16->32 mult.
        ret
				</code></pre>
				<aside class="notes">
				Somehow it realizes that it doesn't need 32-bit inputs. It's almost as if the multiplication function is chosen on based on output sizes, not input sizes.
				This is actually much improved over what it used to be. There is a reason why assembler is still quite important for 8-bit microcontrollers.
				</aside>
				</section>
				<section>
				<h2>Multiplication</h2>
				<p>clang-7</p>
				<pre><code data-trim class="x86asm">
_Z3multh: # u16 = u16 * u8
        imull   %esi, %edi
        movl    %edi, %eax
        retq
_Z8mullargehh: # u32 = (u8 cast to u32) * u8
        movl    %edi, %ecx
        movl    %esi, %eax
        imulq   %rcx, %rax
        retq
				</code></pre>
				<p>Is there a good reason these are different at all??</p>
				<aside class="notes">
				I could imagine some issues around partial register stalls, but it doesn't feel convincing... Or does the ABI not define the high 32 bit of the register?? But that would only be an issue with imulq, imull would do here!
				</aside>
				</section>
				<section>
				<h2>Multiplication</h2>
				<p>gcc 8</p>
				<pre><code data-trim class="x86asm">
_Z3multh: # u16 = u16 * u8
        movzbl  %sil, %eax
        imull   %edi, %eax
        ret
_Z8mullargehh: # u32 = (u8 cast to u32) * u8
        movzbl  %dil, %eax
        movzbl  %sil, %edi
        imulq   %rdi, %rax
        ret
				</code></pre>
				<p>Essentially matches clang</p>
				</section>
				<section>
				<h2>Multiplication</h2>
				<p>gcc 8, AArch64</p>
				<pre><code data-trim class="armasm">
_Z3multh: # u16 = u16 * u8
        and     w1, w1, 255
        and     w0, w0, 65535
        mul     w0, w0, w1
        ret
_Z8mullargehh: # u32 = (u8 cast to u32) * u8
        and     w1, w1, 255
        and     w0, w0, 255
	umull   x0, w0, w1  # note: 32x32 -> 64 multiply
        ret
				</code></pre>
				<p>Wait what?</p>
				<aside class="notes">
				Could ABI be to blame and the high bits really need to be masked? Let's see and compare with clang...
				</aside>
				</section>
				<section>
				<h2>Multiplication</h2>
				<p>clang 7, AArch64</p>
				<pre><code data-trim class="armasm">
_Z3multh: # u16 = u16 * u8
        and     w8, w1, #0xff
        mul     w0, w8, w0
        ret
_Z8mullargehh: # u32 = (u8 cast to u32) * u8
        and     x8, x0, #0xff
        and     x9, x1, #0xff
	mul     x0, x9, x8  # 64x64 -> 64 multiply
        ret
				</code></pre>
				<p>Ok... One masking less, but pointless 64 bit multiply?</p>
				<aside class="notes">
				Conclusion: small types and casts seem to BADLY confuse compilers
				</aside>
				</section>
				</section>


				<section>
				<h1>Thanks!</h1>
				<h2>Questions?</h2>
				<p>More examples and slides: <a href="https://github.com/rdoeffinger/compilertests">https://github.com/rdoeffinger/compilertests</a></p>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
